{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803ac4b6-bc87-4f3e-920c-6259f52a5b67",
   "metadata": {},
   "source": [
    "# Interactive UI demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d799d-2296-4810-af0d-11a386f38689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\AI MSc\\CR\\CR-final\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32693c44-50bb-4d16-847c-ecd13791ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ui\n",
    "importlib.reload(ui)\n",
    "\n",
    "\n",
    "import pybullet as p\n",
    "from ui import RobotEnvUI\n",
    "from llm_utils import prompt_pick_and_place_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b056d86-fa99-4433-add9-e95dec2e5328",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m22\u001b[39m\n\u001b[0;32m      8\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m---> 10\u001b[0m demo \u001b[38;5;241m=\u001b[39m \u001b[43mRobotEnvUI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualise_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\CR-final\\ui.py:93\u001b[0m, in \u001b[0;36mRobotEnvUI.__init__\u001b[1;34m(self, n_objects, n_action_attempts, n_grasp_attempts, visualise_grasps, visualise_clip, ground_truth_segm, clip_prompt_eng, clip_this_is, seed)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mdummy_simulation_steps(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# load GR-ConvNet grasp synthesis network\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrasp_generator \u001b[38;5;241m=\u001b[39m \u001b[43mload_grasp_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcamera\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_grasp_attempts \u001b[38;5;241m=\u001b[39m n_grasp_attempts\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_prompt_eng \u001b[38;5;241m=\u001b[39m clip_prompt_eng\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\CR-final\\grconvnet.py:199\u001b[0m, in \u001b[0;36mload_grasp_generator\u001b[1;34m(camera, checkpoint)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_grasp_generator\u001b[39m(camera, checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_19_iou_0.98\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m \tfig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m--> 199\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGraspGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEPTH_RADIUS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGR_ConvNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\CR-final\\grconvnet.py:30\u001b[0m, in \u001b[0;36mGraspGenerator.__init__\u001b[1;34m(self, net_path, camera, depth_radius, fig, IMG_WIDTH, network, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m cwd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m     29\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m get_device(force_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\venv\\lib\\site-packages\\torch\\serialization.py:1114\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\venv\\lib\\site-packages\\torch\\serialization.py:1348\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1347\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1348\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\AI MSc\\CR\\venv\\lib\\site-packages\\torch\\serialization.py:1149\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'inference'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "seed = 22\n",
    "random.seed(seed)\n",
    "\n",
    "demo = RobotEnvUI(5, seed=seed, visualise_clip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb6cb1-0b67-4e40-8f87-bebe47d09d61",
   "metadata": {},
   "source": [
    "Types of instructions:\n",
    "* <ins>put `[obj`] in `[region]`</ins>, e.g: *\"put the banana in the top right corner\"*\n",
    "* <ins>put `[obj]` in the tray / clean `[obj]`</ins>, e.g: *\"clean the scissors\"*\n",
    "* <ins>put `[obj]` on the `[rel]` of `[obj]`</ins>, e.g.: *\"put the hammer left of the tomato soup can\"*\n",
    "\n",
    "1. <ins>Available regions</ins>: *top/bottom/right/left side, top/bottom left/right corner, middle*\n",
    "2. <ins>Available relations</ins>: *left/right,behind/front, on*\n",
    "3. <ins>Available objects</ins>: open-vocabulary object descriptions (within CLIP capabilities)\n",
    "\n",
    "Use `visualize_clip=True` to view the CLIP recognition predictions.\n",
    "\n",
    "<ins>Chatbot UI</ins> The UI uses the history of instructions as extra context to the LLM. That makes our agent respond to instructions like: *now put the other fruit ...\", \"undo the previous step\", \"actually, I want it somewhere else\", where the LLM picks up the object/location of interest from the chat history. \n",
    "\n",
    "Control the UI:\n",
    "* `:reset` will reset the robot and the scene to initial state\n",
    "* `:new` will generate a new scene (have to pass `seed=None` when starting the demo for this)\n",
    "* `:clear` Clear the chat history\n",
    "* `:exit`Exit the demo\n",
    "* `your language instruction here`: choose one instruction for the robot to complete\n",
    "\n",
    "Known issues:\n",
    "* the robot thinks it has failed the placing part sometimes and attempt to redo it while it has succeeded\n",
    "* CLIP mis-classifications\n",
    "* The longer the interaction, the more chat history in the prompt and the LLM might start ignoring the prompt examples.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e47d84-0555-4fc9-a680-29bc14485adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prompts\n",
    "prompts_in_order = [\n",
    "    \"put the pear on the tray\",\n",
    "    \"now put the other fruit on the right side of the table\",\n",
    "    \"actually, I want it on the top left corner\",\n",
    "    \"thanks! Now put the hammer in front of the scissors\",\n",
    "    \"actually, clean it for me\"\n",
    "    \"great! Now if you can put the electric tool in the middle\",\n",
    "    \"no, in the bottom side\",\n",
    "    \"wait, undo the last step\",\n",
    "    \"actually, clean it up for me\",\n",
    "    \":exit\"\n",
    "]\n",
    "\n",
    "\n",
    "display(demo.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60091878-9a06-44b4-840d-a30cb6e8e892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
